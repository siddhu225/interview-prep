Awesome â€” hereâ€™s **CAP Theorem** in the same interview-ready style as your Caching/Sharding notes: crisp concepts, practical trade-offs, and what to say in the room.

---

# **CAP Theorem â€” System Design Notes**

## **What CAP Really Says (and what it doesnâ€™t)**

In a **distributed system**, during a **network partition** (messages dropped/links split), you must choose:

* **C â€” Consistency:** every read sees the **latest write** (single, coherent view).
* **A â€” Availability:** every request to a non-failing node **gets a response** (may be stale).
* **P â€” Partition tolerance:** the system **keeps operating despite partitions**.

ğŸ‘‰ In practice, **P is non-negotiable** (partitions happen). So CAP becomes: **when P happens, do we prefer C or A?**

> CAP â€œconsistencyâ€ â‰  ACID consistency. CAP-C = linearizable, single-copy view.

---

## **Mental Model (simple & interviewable)**

* **Normal times (no partition):** you can have both C and A.
* **Partition occurs:** you **cannot** guarantee both simultaneously.

  * **Choose C:** some requests **fail/block** (sacrifice A) to avoid stale reads/writes.
  * **Choose A:** all requests **succeed quickly**, but some reads can be **stale** (sacrifice C).

---

## **When to Prefer Consistency (CP)**

Prioritize **correctness over uptime** under partition:

* **Tickets/Seats/Inventory**: prevent **double-sell**.
* **Financial systems / balances / trades**: prevent **lost/phantom updates**.
* **Critical counters/quotas**: must not exceed limits.

**Trade-offs:** higher latency under partition, possible write blocking, degraded availability.

**Typical tech/approaches:**

* Single leader with **sync replication** (can block).
* **Transactions**; linearizable stores; quorums tuned for C.
* **Spanner** (TrueTime), PostgreSQL/MySQL single primary with strict sync replicas.
* DynamoDB **Strongly consistent reads** mode (per-table/per-call).

**Interview line:**

> â€œDuring a partition, Iâ€™ll **reject or block** writes that risk double-booking. We run **CP** for booking paths; browse/search can remain AP.â€

---

## **When to Prefer Availability (AP)**

Prioritize **liveness + low latency** under partition (accept **eventual consistency**):

* **Social content, profiles, feeds, likes**
* **Catalog/metadata pages**
* **Analytics, counters, activity streams**

**Trade-offs:** temporary staleness, conflict resolution later.

**Typical tech/approaches:**

* **Async replication**, **multi-master**, **CRDTs**, **last-write-wins**, **Dynamo-style** quorum.
* **Cassandra**, DynamoDB (default), Riak, Cosmos DB (weaker models).

**Interview line:**

> â€œProfile reads remain **available** even if a region is partitioned; we accept **stale data** for a short window and converge via async replication.â€

---

## **Blended Designs (real-world nuance)**

Most systems mix modes **by feature**:

* **Ticketmaster:** **CP** for booking; **AP** for event browsing.
* **Tinder:** **CP** for match creation; **AP** for profile/media.
* **E-commerce:** **CP** for checkout/inventory decrement; **AP** for product views, recommendations.

**Interview line:**

> â€œIâ€™ll classify endpoints: *critical writes = CP*, *reads/browse = AP*. SLAs and UX guide per-path choices.â€

---

## **Consistency Models (quick ladder)**

* **Strong (linearizable):** read sees latest committed write.
* **Read-your-own-writes:** user sees own updates immediately (others may see stale).
* **Monotonic reads / causal:** related operations appear in order.
* **Eventual:** replicas converge without timing guarantees.

**How to use in interviews:**

> â€œPublic reads are **eventual**; user dashboards are **read-your-own-writes** to avoid UX confusion.â€

---

## **Design Patterns Mapped to CAP Choice**

### If you choose **C (CP)** under partition

* **Synchronous replication** with **write quorums** (e.g., W+R > N).
* **Leader-based** writes; followers reject writes if leader unreachable.
* **Region fencing / lease-based primaries** to avoid split-brain.
* **Idempotent ops** + retries with backoff (avoid double commit).
* **Fail closed** on stale leadership.

**Operational knobs:**

* Quorum sizes (N,R,W), **strict majority** for writes.
* **Circuit breakers** when quorum not met â†’ degrade gracefully.

### If you choose **A (AP)** under partition

* **Async replication**, **multi-writer** acceptance.
* **Conflict resolution:** LWW timestamps, vector clocks, **CRDTs**, application merges.
* **Background reconciliation** / CDC pipelines.
* **Client-side tolerance** (stale UI badges, â€œupdated moments agoâ€).

**Operational knobs:**

* **Short TTLs**, **invalidate-then-refresh** after partition heals.
* **Versioning** (ETags) + **conditional writes** to detect conflicts.

---

## **Failure & UX Playbook**

* **Stale UI risk:** add â€œlast updatedâ€ hints; gray badges; optimistic updates with confirmation.
* **Write conflicts:** surface a **resolve flow** (merge dialog) or enforce **single-writer** per entity.
* **Partition detection:** health checks between regions, quorum failures, leader lease expiry.
* **Degradation plan:** read-only mode, queue writes, or route to nearest healthy region.

---

## **Quick Decision Tree (say this out loud)**

1. **Will stale data cause harm?**

   * **Yes â†’ CP** for that path.
   * **No â†’ AP** with eventual convergence.
2. **Is user-perceived freshness required (self-view)?**

   * Add **read-your-own-writes** cache/session pinning.
3. **Whatâ€™s the partition blast radius?**

   * Region-local **single writer**; cross-region **AP reads**; reconcile offline.

---

## **Common Interview Pitfalls (and fixes)**

* **Saying â€œwe need both C and A under partition.â€**
  â†’ Acknowledge CAP: â€œUnder partition, we **prefer C** for X, **A** for Y.â€
* **Equating CAP-C with ACID**
  â†’ Clarify linearizability vs transactional constraints.
* **Ignoring UX during partitions**
  â†’ Offer a **read-only** mode or **queue & confirm** model.
* **Global CP everywhere** (needless latency)
  â†’ **Scope CP** to the **minimal critical write path**.

---

## **Sample Interview Script (90 seconds)**

> â€œFor non-functional requirements, letâ€™s pick our CAP posture. Under partitions, **checkout & inventory** must avoid oversells, so theyâ€™re **CP**: leader-based writes, quorum commit, fail closed if quorum is lost. Browsing, search, reviews are **AP**: we accept temporary staleness, use async replication and cache, and reconcile via CDC. Users get **read-your-own-writes** so they immediately see their changes. Across regions we fence leaders with leases to prevent split-brain, and degrade to **read-only** if quorum isnâ€™t met. That balances correctness where it matters and availability everywhere else.â€

---

## **Cheat Lines**

* â€œ**P is mandatory**, so the choice is **C vs A during partitions**.â€
* â€œ**CP for money/tickets; AP for feed/content**.â€
* â€œ**Read-your-own-writes** hides staleness for the author without forcing global CP.â€
* â€œWeâ€™ll **fail closed** for booking; **serve stale** for browse; **reconcile** via CDC.â€

---
